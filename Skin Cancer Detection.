{"cells":[{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VAKS5vSMexqd","executionInfo":{"status":"ok","timestamp":1737678899364,"user_tz":-330,"elapsed":6014,"user":{"displayName":"akshat shukla","userId":"01979925258072508501"}},"outputId":"c3dc6049-a013-4d8e-d2f5-4ae59b57a4a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Data source import complete.\n"]}],"source":["# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n","# THEN FEEL FREE TO DELETE THIS CELL.\n","# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n","# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n","# NOTEBOOK.\n","import kagglehub\n","nodoubttome_skin_cancer9_classesisic_path = kagglehub.dataset_download('nodoubttome/skin-cancer9-classesisic')\n","\n","print('Data source import complete.')\n"]},{"cell_type":"markdown","metadata":{"id":"F5pPqgc2exqe"},"source":["# Problem Statement\n","\n","Problem statement: To build a CNN based model which can accurately detect melanoma. Melanoma is a type of cancer that can be deadly if not detected early. It accounts for 75% of skin cancer deaths. A solution which can evaluate images and alert the dermatologists about the presence of melanoma has the potential to reduce a lot of manual effort needed in diagnosis."]},{"cell_type":"markdown","metadata":{"id":"lyV65PDaexqf"},"source":["# Importing libs"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"aSfjD144exqg","executionInfo":{"status":"ok","timestamp":1737678899364,"user_tz":-330,"elapsed":8,"user":{"displayName":"akshat shukla","userId":"01979925258072508501"}}},"outputs":[],"source":["import pathlib\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import os\n","import PIL\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n","from tensorflow.keras.optimizers import Adam # - Works\n","import random\n","from glob import glob\n","import seaborn as sns\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","import matplotlib.pyplot as plt\n","import matplotlib.image as img\n","import warnings\n","warnings.filterwarnings('ignore')\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"DfJX9LHIexqg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737678899364,"user_tz":-330,"elapsed":8,"user":{"displayName":"akshat shukla","userId":"01979925258072508501"}},"outputId":"377044f9-d692-4c37-feea-c7490b209b7d"},"outputs":[{"output_type":"stream","name":"stdout","text":["[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"]}],"source":["gpus = tf.config.experimental.list_physical_devices('GPU')\n","print(gpus)\n","try:\n","    tf.config.experimental.set_memory_growth = True\n","except Exception as ex:\n","    print(e)"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"M1Z5iXTGexqh","executionInfo":{"status":"ok","timestamp":1737678899364,"user_tz":-330,"elapsed":7,"user":{"displayName":"akshat shukla","userId":"01979925258072508501"}}},"outputs":[],"source":["! rm -rf /kaggle/working/data/"]},{"cell_type":"markdown","metadata":{"id":"BTMo8LQAexqh"},"source":["# Reading input data"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"VqzEPkzXexqi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737678899364,"user_tz":-330,"elapsed":6,"user":{"displayName":"akshat shukla","userId":"01979925258072508501"}},"outputId":"da7987d0-58b3-4d02-ac9e-de05a0a206d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","0\n"]}],"source":["image_count_train = len(list(data_dir_train.glob('*/*.jpg')))\n","print(image_count_train)\n","image_count_test = len(list(data_dir_test.glob('*/*.jpg')))\n","print(image_count_test)"]},{"cell_type":"markdown","metadata":{"id":"tiRDPPUuexqi"},"source":["# Prepare the dataset"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"KOcbVPXTexqj","executionInfo":{"status":"ok","timestamp":1737678901306,"user_tz":-330,"elapsed":2,"user":{"displayName":"akshat shukla","userId":"01979925258072508501"}}},"outputs":[],"source":["batch_size = 32\n","img_height = 180\n","img_width = 180\n","rnd_seed = 123\n","random.seed(rnd_seed)"]},{"source":["# Assuming your data is in the Kaggle input directory, use the following path:\n","data_dir_train = pathlib.Path(\"/kaggle/input/skin-cancer9-classesisic/Skin cancer ISIC The International Skin Imaging Collaboration/Train/\")\n","data_dir_test = pathlib.Path(\"/kaggle/input/skin-cancer9-classesisic/Skin cancer ISIC The International Skin Imaging Collaboration/Test/\")"],"cell_type":"code","metadata":{"id":"8CFXBi6-vB7h","executionInfo":{"status":"ok","timestamp":1737678925482,"user_tz":-330,"elapsed":873,"user":{"displayName":"akshat shukla","userId":"01979925258072508501"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","execution_count":36,"metadata":{"id":"9nlo0CfAexqj","colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"status":"error","timestamp":1737678953581,"user_tz":-330,"elapsed":11,"user":{"displayName":"akshat shukla","userId":"01979925258072508501"}},"outputId":"ba4cf6ef-0dc0-41af-c286-60ad2f9c6531"},"outputs":[{"output_type":"error","ename":"NotFoundError","evalue":"Could not find directory /kaggle/input/skin-cancer9-classesisic/Skin cancer ISIC The International Skin Imaging Collaboration/Train","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-187d696c87a3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mdata_dir_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/image_dataset_utils.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     image_paths, labels, class_names = dataset_utils.index_directory(\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links, verbose)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"inferred\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0msubdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mlist_directory_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    766\u001b[0m   \"\"\"\n\u001b[1;32m    767\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m     raise errors.NotFoundError(\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0mnode_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0mop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotFoundError\u001b[0m: Could not find directory /kaggle/input/skin-cancer9-classesisic/Skin cancer ISIC The International Skin Imaging Collaboration/Train"]}],"source":["train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","  data_dir_train,\n","  validation_split=0.2,\n","  subset=\"training\",\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"imP5WjIgexqk"},"outputs":[],"source":["val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","  data_dir_train,\n","  validation_split=0.2,\n","  subset=\"validation\",\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P4EyeTGCexqk"},"outputs":[],"source":["test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","  data_dir_test,\n","  validation_split=0.9,\n","  subset=\"validation\",\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hkhuQlSQexqk"},"outputs":[],"source":["class_names = train_ds.class_names\n","print(class_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vg7CgvZdexqk"},"outputs":[],"source":["num_classes = len(class_names)\n","plt.figure(figsize=(10,10))\n","for i in range(num_classes):\n","  plt.subplot(3,3,i+1)\n","  image = img.imread(str(list(data_dir_train.glob(class_names[i]+'/*.jpg'))[1]))\n","  plt.title(class_names[i])\n","  plt.imshow(image)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HEgk-Gcuexqk"},"outputs":[],"source":["for image_batch, labels_batch in train_ds.take(1):\n","    print(image_batch.shape)\n","    print(labels_batch.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ca6QRxW0exqk"},"outputs":[],"source":["AUTOTUNE = tf.data.experimental.AUTOTUNE\n","train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n","val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"]},{"cell_type":"markdown","metadata":{"id":"fdSS8-x7exql"},"source":["# Model 1 : standard Model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QIIV-QIzexql"},"outputs":[],"source":["num_classes = 9\n","model = Sequential([layers.experimental.preprocessing.Rescaling \\\n","                    (1.0/255,input_shape=(img_height,img_width,3))])\n","\n","model.add(Conv2D(32, 3,padding=\"same\",activation='relu'))\n","model.add(MaxPool2D())\n","\n","model.add(Conv2D(64, 3,padding=\"same\",activation='relu'))\n","model.add(MaxPool2D())\n","\n","model.add(Conv2D(128, 3,padding=\"same\",activation='relu'))\n","model.add(MaxPool2D())\n","\n","model.add(Conv2D(256, 3,padding=\"same\",activation='relu'))\n","model.add(MaxPool2D())\n","\n","model.add(Conv2D(512, 3,padding=\"same\",activation='relu'))\n","model.add(MaxPool2D())\n","\n","model.add(Flatten())\n","model.add(Dense(1024,activation=\"relu\"))\n","model.add(Dense(units=num_classes, activation= 'softmax'))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"91ONhmEWexql"},"outputs":[],"source":["opt = Adam(lr=0.001)\n","model.compile(optimizer= opt,\n","              loss= SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_AAFFZMfexql"},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u5gtKW6Rexql"},"outputs":[],"source":["epochs = 25\n","history = model.fit(\n","  train_ds,\n","  validation_data=val_ds,\n","  epochs=epochs\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ydzwlwfqexql"},"outputs":[],"source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs_range = range(epochs)\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"0AELlonpexql"},"source":["## Findings :\n","*   The model is overfitting because we can see the difference in accuracy in training data & accuracy in the validation data that is almost 20%.\n","\n","*   The training accuracy is just around 70-75% with 25 epochos and the model is yet to learn the many features.\n","\n","*  data imbalance might be causing the bais to the model."]},{"cell_type":"markdown","metadata":{"id":"CuShZ7mhexql"},"source":["# Model 2 : Data Augumentation with drop out layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AcU2Vh0wexql"},"outputs":[],"source":["data_augmentation = keras.Sequential(\n","  [\n","    layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\",\n","                                                 input_shape=(img_height,\n","                                                              img_width,\n","                                                              3)),\n","    layers.experimental.preprocessing.RandomRotation(0.2),\n","    layers.experimental.preprocessing.RandomZoom(0.2),\n","  ]\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_UCcEn9vexql"},"outputs":[],"source":["plt.figure(figsize=(10, 10))\n","for images, _ in train_ds.take(1):\n","  for i in range(9):\n","    augmented_images = data_augmentation(images)\n","    ax = plt.subplot(3, 3, i + 1)\n","    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n","    plt.axis(\"off\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-e_12pY-exqm"},"outputs":[],"source":["num_classes = 9\n","model = Sequential(data_augmentation)\n","model.add(layers.experimental.preprocessing.Rescaling(1.0/255,input_shape=(img_height,img_width,3)))\n","\n","model.add(Conv2D(32, 3,padding=\"same\",activation='relu'))\n","model.add(MaxPool2D())\n","\n","model.add(Conv2D(64, 3,padding=\"same\",activation='relu'))\n","model.add(MaxPool2D())\n","\n","model.add(Conv2D(128, 3,padding=\"same\",activation='relu'))\n","model.add(MaxPool2D())\n","model.add(Dropout(0.15))\n","\n","model.add(Conv2D(256, 3,padding=\"same\",activation='relu'))\n","model.add(MaxPool2D())\n","model.add(Dropout(0.20))\n","\n","model.add(Conv2D(512, 3,padding=\"same\",activation='relu'))\n","model.add(MaxPool2D())\n","model.add(Dropout(0.25))\n","\n","model.add(Flatten())\n","model.add(Dense(1024,activation=\"relu\"))\n","model.add(Dense(units=num_classes, activation= 'softmax'))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kYE5zsbGexqm"},"outputs":[],"source":["opt = Adam(lr=0.001)\n","model.compile(optimizer=opt,\n","              loss= SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ggKYU3ZTexqm"},"outputs":[],"source":["epochs = 25\n","history = model.fit(\n","  train_ds,\n","  validation_data=val_ds,\n","  epochs=epochs\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fb8PyAP6exqm"},"outputs":[],"source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs_range = range(epochs)\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"zR911QPyexqm"},"source":["## Findings\n","* With data agumenatation and drop layer, the overfitting of the model is adressed to great extend. Earlier the train and validation accuracy difference was nearly 20%, with latest approach it's reduced to 2-3%.\n","\n","* The accuracy of the model is compromised heavily and decreased by fair bit from previous venilla model.\n","\n","* Considering above 2 points, there is still a scope of lot of improvement of the model.\n"]},{"cell_type":"markdown","metadata":{"id":"NO4jOQmiexqm"},"source":["# Analysing the class imbalance of the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2uxH4ReXexqm"},"outputs":[],"source":["num_classes = len(class_names)\n","total = 0\n","all_count = []\n","class_name = []\n","for i in range(num_classes):\n","  count = len(list(data_dir_train.glob(class_names[i]+'/*.jpg')))\n","  total += count\n","print(\"total training image count = {} \\n\".format(total))\n","print(\"-------------------------------------\")\n","for i in range(num_classes):\n","  count = len(list(data_dir_train.glob(class_names[i]+'/*.jpg')))\n","  print(\"Class name = \",class_names[i])\n","  print(\"count      = \",count)\n","  print(\"proportion = \",count/total)\n","  print(\"-------------------------------------\")\n","  all_count.append(count)\n","  class_name.append(class_names[i])\n","\n","temp_df = pd.DataFrame(list(zip(all_count, class_name)), columns = ['count', 'class_name'])\n","sns.barplot(data=temp_df, y=\"count\", x=\"class_name\")\n","plt.xticks(rotation=90)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"trxXhHQiexqn"},"source":["## Findings\n","Data is hevily imbalance and hence due to that results and predictions will be baised."]},{"cell_type":"markdown","metadata":{"id":"05qIFKViexqn"},"source":["# Augmentor : Class balance\n","\n","Using Augmentor (https://augmentor.readthedocs.io/en/master/) to create the equal distribution of the class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kj9waUreexqn"},"outputs":[],"source":["!pip install Augmentor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"meBeJu08exqn"},"outputs":[],"source":["path_to_training_dataset = '../input/skin-cancer9-classesisic/Skin cancer ISIC The International Skin Imaging Collaboration/Train/'\n","import Augmentor\n","for i in class_names:\n","    p = Augmentor.Pipeline(path_to_training_dataset + i, output_directory='/kaggle/working/data/'+i+'/output/')\n","    p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n","    p.sample(1000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fMSE1LoGexqs"},"outputs":[],"source":["output_dir = pathlib.Path('/kaggle/working/data/')\n","image_count_train = len(list(output_dir.glob('*/output/*.jpg')))\n","print(image_count_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"phfCGd-Nexqs"},"outputs":[],"source":["num_classes = len(class_names)\n","total = 0\n","all_count = []\n","class_name = []\n","\n","for i in range(num_classes):\n","  count = len(list(output_dir.glob(class_names[i]+'/output/*.jpg')))\n","  total += count\n","print(\"total training image count = {} \\n\".format(total))\n","print(\"-------------------------------------\")\n","for i in range(num_classes):\n","  count = len(list(output_dir.glob(class_names[i]+'/output/*.jpg')))\n","  print(\"Class name = \",class_names[i])\n","  print(\"count      = \",count)\n","  print(\"proportion = \",count/total)\n","  print(\"-------------------------------------\")\n","  all_count.append(count)\n","  class_name.append(class_names[i])\n","\n","\n","temp_df = pd.DataFrame(list(zip(all_count, class_name)), columns = ['count', 'class_name'])\n","sns.barplot(data=temp_df, y=\"count\", x=\"class_name\")\n","plt.xticks(rotation=90)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"GLqXyrcIexqs"},"source":["# Model 3 : Model with Class balance data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iNExthEgexqs"},"outputs":[],"source":["train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","  output_dir,\n","  seed=123,\n","  validation_split = 0.2,\n","  subset = 'training',\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QXyjOdKDexqs"},"outputs":[],"source":["val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","  output_dir,\n","  seed=123,\n","  validation_split = 0.2,\n","  subset = 'validation',\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8BMHHvtoexqt"},"outputs":[],"source":["print(train_ds.class_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6_wmb2zDexqt"},"outputs":[],"source":["num_classes = 9\n","model = Sequential([layers.experimental.preprocessing.Rescaling(1.0/255,input_shape=(img_height,img_width,3))])\n","\n","model.add(Conv2D(32, 3,padding=\"same\",activation='relu'))\n","model.add(MaxPool2D())\n","\n","model.add(Conv2D(64, 3,padding=\"same\",activation='relu'))\n","model.add(MaxPool2D())\n","\n","model.add(Conv2D(128, 3,padding=\"same\",activation='relu'))\n","model.add(MaxPool2D())\n","model.add(Dropout(0.15))\n","\n","model.add(Conv2D(256, 3,padding=\"same\",activation='relu'))\n","model.add(MaxPool2D())\n","model.add(Dropout(0.20))\n","\n","model.add(Conv2D(512, 3,padding=\"same\",activation='relu'))\n","model.add(MaxPool2D())\n","model.add(Dropout(0.25))\n","\n","model.add(Flatten())\n","model.add(Dense(1024,activation=\"relu\"))\n","model.add(Dense(units=num_classes, activation= 'softmax'))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g3HJ4DfVexqt"},"outputs":[],"source":["opt = Adam(lr=0.001)\n","model.compile(optimizer= opt,\n","              loss = SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B5HZq-DWexqt"},"outputs":[],"source":["epochs = 25\n","history = model.fit(\n","  train_ds,\n","  validation_data=val_ds,\n","  epochs=epochs\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Z4fpoHmexqt"},"outputs":[],"source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs_range = range(epochs)\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Z5ZhU9l9exqt"},"source":["## Findings :\n","* After rebalance/resampling of the data (that gave equal proportion of data )and raised the accuray of the mdoel to 90%. This addressed the low accurty problem.\n","\n","*  overfitting probelm is adressed and now difference between train and val set is nearly 4-5% diff.\n","\n","* with these results it's conclusive that current module with rebalanced data is the best module.\n"]},{"cell_type":"markdown","metadata":{"id":"-pe6yQa3exqu"},"source":["# Evaluation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qe4bmOo3exqu"},"outputs":[],"source":["#Create a file to save models\n","top_model_weights_path = '/kaggle/working/cnn_fc_model.h5'\n","model.save_weights(top_model_weights_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"udA7-Kxlexqv"},"outputs":[],"source":["(eval_loss, eval_accuracy) = model.evaluate(test_ds, batch_size=batch_size, \\\n","                                            verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OIDp5ARWexqw"},"outputs":[],"source":["print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100))\n","print(\"[INFO] Loss: {}\".format(eval_loss))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XyaSJNBzexqw"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/skin-cancer-detection-4b6d8532-2c6f-4637-84b6-37db9438929d.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20241225/auto/storage/goog4_request&X-Goog-Date=20241225T154129Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=1921480578631aa1a544f00707a5fd5b86a49a16342b6ded460f1c9e144f8df264a2563db7cca83ec6b0551269a2ffd1b55f1782b229749500115508d5080fe2df24603438762bccbe4109902a55d9634f86ad25cd382d2094afbcfb54f02de7e85cbeca1c196e0e15391e9c2941f691d038d3acb9c507521d0b8933cfd8cb9d9705810ae5df1a79c7512119c6a8e49d055ea8cbcf5554a2b1ca2d0b06e5b49288a1936297e8a6f57bf75ba5ed3075d81bd3d3700f38e7dbaa0f3e85f120197e98f0c256581515285c367bebc924ac4d06404d49ec119eae4163178ba841877243ecdb757dd682a5c48378cc09e72ec21771eabc2c6eaf48a807da6382b378e3","timestamp":1735141328255}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":0}